# =============================================================================
# PostgreSQL 16 — Streaming Replication Configuration for Read Replicas
# EduSphere Platform — 100,000+ concurrent users
# =============================================================================
# Apply to: replica servers only (standby nodes).
# Primary server also needs wal_level, max_wal_senders, wal_keep_size.
# =============================================================================

# -----------------------------------------------------------------------------
# WAL (Write-Ahead Log) — Replication Source
# These settings are required on the PRIMARY to enable streaming replication.
# Copy this section to postgresql.conf on the primary.
# -----------------------------------------------------------------------------

# Enable replica-level WAL detail. Required for streaming replication.
# Options: minimal < replica < logical. Use "replica" — "logical" has overhead.
wal_level = replica

# Maximum number of WAL sender processes allowed to connect from standby nodes.
# Set to (number_of_replicas + 1) as a buffer for pg_basebackup jobs.
max_wal_senders = 5

# Minimum amount of WAL data retained on primary to allow slow replicas to catch up.
# Replicas that fall behind by more than this will require a full re-sync.
# 1GB is appropriate for bursts at 100k concurrent users.
wal_keep_size = 1GB

# -----------------------------------------------------------------------------
# Standby / Hot Standby — Settings for REPLICA servers
# These settings enable the replica to serve read queries while replicating.
# -----------------------------------------------------------------------------

# Allow read queries on a standby server during recovery.
# Without this, the replica is unavailable for reads.
hot_standby = on

# Prevent query cancellations on the standby due to conflicts with HOT cleanup
# on the primary. The primary will delay HOT cleanup to avoid interrupting
# replica read queries — important for analytics workloads.
hot_standby_feedback = on

# Maximum delay allowed for conflict resolution on the hot standby.
# 0 = no limit (replica will cancel queries immediately on conflict).
# 30s gives analytics queries time to complete before conflict resolution.
max_standby_streaming_delay = 30s

# Maximum delay during file-based recovery (pg_restore, WAL archive replay).
max_standby_archive_delay = 30s

# -----------------------------------------------------------------------------
# Recovery / Replication Target
# -----------------------------------------------------------------------------

# Follow the latest timeline after a failover. This ensures the replica
# automatically catches up with the new primary after a promotion event.
recovery_target_timeline = 'latest'

# Connection string for the replication stream from primary.
# PLACEHOLDER — replace with actual primary address before deploying.
# Uses the dedicated 'replicator' role with scram-sha-256 auth (see pg_hba.conf).
# Format: 'host=<primary-host> port=5432 user=replicator password=<secret> sslmode=require'
primary_conninfo = 'host=postgres-primary port=5432 user=replicator password=REPLACE_WITH_REPLICATOR_PASSWORD sslmode=require'

# Standby signal file tells PostgreSQL to enter standby mode.
# Create this file on replica: touch $PGDATA/standby.signal
# (PostgreSQL 12+ uses standby.signal instead of recovery.conf)

# -----------------------------------------------------------------------------
# Monitoring & Replication Lag
# -----------------------------------------------------------------------------

# Frequency at which the standby sends status updates to the primary.
# Lower = faster lag detection. 10s is a good balance for monitoring.
wal_receiver_status_interval = 10s

# Maximum wait time for new WAL data before the standby reconnects.
wal_receiver_timeout = 60s

# Time to wait before attempting to reconnect after a failed WAL receiver.
wal_retrieve_retry_interval = 5s

# -----------------------------------------------------------------------------
# Performance Tuning for Read Workloads
# Replicas serve read-heavy analytics and GraphQL read queries.
# -----------------------------------------------------------------------------

# Allow use of query parallelism on the replica.
max_parallel_workers_per_gather = 4

# Effective cache size hint for the query planner (not allocated — advisory only).
# Set to 75% of available RAM on replica nodes.
# Adjust per instance: 12GB for 16GB RAM nodes.
effective_cache_size = 12GB

# Shared buffer cache — typically 25% of RAM.
shared_buffers = 4GB

# Cost factor for random page I/O (lower on NVMe/SSD replicas).
random_page_cost = 1.1
